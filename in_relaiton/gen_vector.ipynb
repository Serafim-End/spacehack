{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/python:/root/python:\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 s, sys: 336 ms, total: 13.3 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('web_0_300_20.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 148 ms, sys: 40 ms, total: 188 ms\n",
      "Wall time: 188 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = dict(zip(model.index2word, model.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenses = [unicode('мама мыла раму папа мыл паркет', encoding='utf-8'),\n",
    "             unicode('ехал грека через реку видит грека в реке хуй', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'ADJF': '_ADJ',\n",
    "    'NOUN': '_NOUN',\n",
    "    'INFN': '_VERB',\n",
    "}\n",
    "\n",
    "def prepare_word(w, mapping, word2vec_model):\n",
    "    w = morph.parse(w)[0]\n",
    "    if w.tag.POS in mapping:\n",
    "        new_w = '{}{}'.decode('utf-8').format(w.normal_form, mapping[w.tag.POS])\n",
    "        v = vector(new_w, word2vec_model)  \n",
    "        return v, new_w\n",
    "    return (None, None)\n",
    "\n",
    "def w2v_transformation(data, word2vec_model):\n",
    "    \"\"\"\n",
    "    transform the data - it can be pandas format or just array\n",
    "    :param data: data - pandas, numpy, list\n",
    "    :param word2vec_model\n",
    "    :return: transformed_data\n",
    "    \"\"\"\n",
    "    dim = len(word2vec_model.itervalues().next())\n",
    "\n",
    "    train_w2v_tfidf = []\n",
    "    for i, s in enumerate(data):\n",
    "        words = s.split(' ')\n",
    "\n",
    "        words_vector = None\n",
    "        words_count = 0\n",
    "\n",
    "        new_s = []\n",
    "        sentence_vector = []\n",
    "        for w in words:\n",
    "            v, new_w = prepare_word(w, mapping, word2vec_model)\n",
    "            if new_w:\n",
    "                new_s.append(new_w)\n",
    "            \n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(new_s)\n",
    "            \n",
    "        max_idf = max(tfidf.idf_)\n",
    "        word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        )\n",
    "            \n",
    "        train_w2v_tfidf.append(\n",
    "            np.array(\n",
    "                np.mean(\n",
    "                    [word2vec_model[w] * word2weight[w] for w in new_s if w in word2vec_model]\n",
    "                    or [np.zeros(dim)],\n",
    "                    axis=0\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "#     train_vectors = np.array(train_vectors)\n",
    "    return np.array(train_w2v_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector(q, word2vec_model):\n",
    "    \"\"\"\n",
    "    transform a single word to word2vec model - 500 digits\n",
    "    :param q: just a word from w2v vocabulary\n",
    "    :param word2vec_model: word2vec model\n",
    "    :return: vector from 500 digits, numpy\n",
    "    \"\"\"\n",
    "    qf = q\n",
    "\n",
    "    if q not in word2vec_model:\n",
    "        candidates_set = set()\n",
    "\n",
    "        candidates_set.add(q.upper())\n",
    "        candidates_set.add(q.lower())\n",
    "        candidates_set.add(q.capitalize())\n",
    "\n",
    "        no_results = True\n",
    "        for candidate in candidates_set:\n",
    "            if candidate in word2vec_model:\n",
    "                qf = candidate\n",
    "                no_results = False\n",
    "                break\n",
    "\n",
    "        if no_results:\n",
    "            # obvious that not all elements\n",
    "            # in corpus will from our vocabulary\n",
    "            return None\n",
    "\n",
    "    raw_vector = word2vec_model[qf]\n",
    "    return raw_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_tfidf = w2v_transformation([sentenses[0]], w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = json.loads(\"\"\"{\n",
    "    \"friends\": {\n",
    "        \"count\": 0,\n",
    "        \"items\": []\n",
    "    },\n",
    "    \n",
    "    \"basic\": {\n",
    "        \"id\": 30343797,\n",
    "        \"first_name\": \"Artash\",\n",
    "        \"last_name\": \"Muradyan\",\n",
    "        \"sex\": 2,\n",
    "        \"country\": {\n",
    "            \"id\": 1,\n",
    "            \"title\": \"Russia\"\n",
    "        },\n",
    "        \"photo\": \"https://vk.com/images/camera_50.png\",\n",
    "        \"university\": 0,\n",
    "        \"university_name\": \"\",\n",
    "        \"faculty\": 0,\n",
    "        \"faculty_name\": \"\",\n",
    "        \"graduation\": 0,\n",
    "        \"home_town\": \"\",\n",
    "        \"relation\": 0,\n",
    "        \"about\": \"\"\n",
    "    },\n",
    "    \"wall\": {\n",
    "        \"count\": 0,\n",
    "        \"items\": []\n",
    "    },\n",
    "    \"id\": 30343797,\n",
    "    \"groups\": null\n",
    "}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'basic': {u'about': u'',\n",
      "            u'country': {u'id': 1, u'title': u'Russia'},\n",
      "            u'faculty': 0,\n",
      "            u'faculty_name': u'',\n",
      "            u'first_name': u'Artash',\n",
      "            u'graduation': 0,\n",
      "            u'home_town': u'',\n",
      "            u'id': 30343797,\n",
      "            u'last_name': u'Muradyan',\n",
      "            u'photo': u'https://vk.com/images/camera_50.png',\n",
      "            u'relation': 0,\n",
      "            u'sex': 2,\n",
      "            u'university': 0,\n",
      "            u'university_name': u''},\n",
      " u'friends': {u'count': 0, u'items': []},\n",
      " u'groups': None,\n",
      " u'id': 30343797,\n",
      " u'wall': {u'count': 0, u'items': []}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_mapping = {}\n",
    "interest_mapping = {}\n",
    "name_counter = 0\n",
    "\n",
    "\n",
    "def get_main_ohe(o):\n",
    "    v = []\n",
    "    \n",
    "    basics = o['basic']\n",
    "    \n",
    "    v.append(basics['id'])\n",
    "    \n",
    "#     if basics['first_name'] in mapping:\n",
    "#         v.append(mapping[basics['first_name']])\n",
    "#     else:\n",
    "#         name_counter += 1\n",
    "#         mapping[basics['first_name']] = name_counter\n",
    "#         v.append(name_counter)\n",
    "        \n",
    "    v.append(basics['sex'])\n",
    "    \n",
    "    city = basics['city']['id'] if 'city' in basics else -1\n",
    "    v.append(city)\n",
    "\n",
    "    country = basics['country']['id'] if 'country' in basics else -1\n",
    "    v.append(country)\n",
    "    \n",
    "    photo = 1 if 'photo' in basics else 0\n",
    "    v.append(photo)\n",
    "    \n",
    "    birthdate = (time.mktime(datetime.strptime(basics['bdate'], '%d.%m.%Y').timetuple())\n",
    "                 if 'bdate' in basics else 788907600.0)\n",
    "    v.append(birthdate)\n",
    "    \n",
    "    relation = basics['relation'] if 'relation' in basics else -1\n",
    "    v.append(relation)\n",
    "    \n",
    "    university = basics['university'] if 'university' in basics else -1\n",
    "    \n",
    "    about = w2v_transformation(basics['about'], w2v) if 'about' in basics else np.zeros\n",
    "    if not about:\n",
    "        about = np.zeros((300,))\n",
    "\n",
    "    v += list(about)\n",
    "    \n",
    "    return np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   452  100   452    0     0    858      0 --:--:-- --:--:-- --:--:--   857\n",
      "-en {\"interests\": [\"\\u042e\\u043c\\u043e\\u0440\", \"\\u041d\\u043e\\u0432\\u043e\\u0441\\u0442\\u0438\", \"\\u041b\\u0438\\u0442\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u0430\", \"\\u041f\\u0443\\u0442\\u0435\\u0448\\u0435\\u0441\\u0442\\u0432\\u0438\\u044f\", \"\\u041c\\u0443\\u0437\\u044b\\u043a\\u0430\"], \"mbti\": {\"probs\": {\"intro_extra\": 0.2446371465921402, \"sensor_intuit\": 0.25243809819221497, \"logic_ethic\": 0.049992892891168594, \"irratio_ratio\": 0.41522619128227234}, \"psy_type\": \"ENFP\"}}\n"
     ]
    }
   ],
   "source": [
    "!echo -en $(curl http://138.201.80.213:5005/get_user_info?uid=25553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datafuel_api(user_id):\n",
    "     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дети - мамы\n"
     ]
    }
   ],
   "source": [
    "print '\\xd0\\x94\\xd0\\xb5\\xd1\\x82\\xd0\\xb8 - \\xd0\\xbc\\xd0\\xb0\\xd0\\xbc\\xd1\\x8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   452  100   452    0     0    845      0 --:--:-- --:--:-- --:--:--   844\n",
      "-en {\"interests\": [\"\\u042e\\u043c\\u043e\\u0440\", \"\\u041d\\u043e\\u0432\\u043e\\u0441\\u0442\\u0438\", \"\\u041b\\u0438\\u0442\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u0430\", \"\\u041f\\u0443\\u0442\\u0435\\u0448\\u0435\\u0441\\u0442\\u0432\\u0438\\u044f\", \"\\u041c\\u0443\\u0437\\u044b\\u043a\\u0430\"], \"mbti\": {\"probs\": {\"intro_extra\": 0.2446371465921402, \"sensor_intuit\": 0.25243809819221497, \"logic_ethic\": 0.049992892891168594, \"irratio_ratio\": 0.41522619128227234}, \"psy_type\": \"ENFP\"}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   452  100   452    0     0    741      0 --:--:-- --:--:-- --:--:--   742\n",
      "-en {\"interests\": [\"\\u042e\\u043c\\u043e\\u0440\", \"\\u041d\\u043e\\u0432\\u043e\\u0441\\u0442\\u0438\", \"\\u041b\\u0438\\u0442\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u0430\", \"\\u041f\\u0443\\u0442\\u0435\\u0448\\u0435\\u0441\\u0442\\u0432\\u0438\\u044f\", \"\\u041c\\u0443\\u0437\\u044b\\u043a\\u0430\"], \"mbti\": {\"probs\": {\"intro_extra\": 0.2446371465921402, \"sensor_intuit\": 0.25243809819221497, \"logic_ethic\": 0.049992892891168594, \"irratio_ratio\": 0.41522619128227234}, \"psy_type\": \"ENFP\"}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   452  100   452    0     0    747      0 --:--:-- --:--:-- --:--:--   748\n",
      "-en {\"interests\": [\"\\u042e\\u043c\\u043e\\u0440\", \"\\u041d\\u043e\\u0432\\u043e\\u0441\\u0442\\u0438\", \"\\u041b\\u0438\\u0442\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u0430\", \"\\u041f\\u0443\\u0442\\u0435\\u0448\\u0435\\u0441\\u0442\\u0432\\u0438\\u044f\", \"\\u041c\\u0443\\u0437\\u044b\\u043a\\u0430\"], \"mbti\": {\"probs\": {\"intro_extra\": 0.2446371465921402, \"sensor_intuit\": 0.25243809819221497, \"logic_ethic\": 0.049992892891168594, \"irratio_ratio\": 0.41522619128227234}, \"psy_type\": \"ENFP\"}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   452  100   452    0     0    777      0 --:--:-- --:--:-- --:--:--   776\n",
      "-en {\"interests\": [\"\\u042e\\u043c\\u043e\\u0440\", \"\\u041d\\u043e\\u0432\\u043e\\u0441\\u0442\\u0438\", \"\\u041b\\u0438\\u0442\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u0430\", \"\\u041f\\u0443\\u0442\\u0435\\u0448\\u0435\\u0441\\u0442\\u0432\\u0438\\u044f\", \"\\u041c\\u0443\\u0437\\u044b\\u043a\\u0430\"], \"mbti\": {\"probs\": {\"intro_extra\": 0.2446371465921402, \"sensor_intuit\": 0.25243809819221497, \"logic_ethic\": 0.049992892891168594, \"irratio_ratio\": 0.41522619128227234}, \"psy_type\": \"ENFP\"}}\n",
      "1 loop, best of 3: 717 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!echo -en $(curl http://138.201.80.213:5005/get_user_info?uid=25553&fields=interests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
